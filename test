import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.OffsetAndMetadata;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.stereotype.Service;

import java.time.Duration;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;
import java.util.Properties;

@Service
public class KafkaOffsetConsumer {

    private static final String TOPIC = "test-topic";
    private static final String BOOTSTRAP_SERVERS = "localhost:9092";
    private static final String GROUP_ID = "group_id";

    private KafkaConsumer<String, String> createKafkaConsumer() {
        Properties props = new Properties();
        props.put("bootstrap.servers", BOOTSTRAP_SERVERS);
        props.put("group.id", GROUP_ID);
        props.put("key.deserializer", StringDeserializer.class.getName());
        props.put("value.deserializer", StringDeserializer.class.getName());
        props.put("enable.auto.commit", "false"); // Manual commit

        return new KafkaConsumer<>(props);
    }

    // Read messages normally
    @KafkaListener(topics = TOPIC, groupId = GROUP_ID)
    public void consume(String message, Acknowledgment acknowledgment) {
        System.out.println("Received message: " + message);
        acknowledgment.acknowledge(); // Manually acknowledge the message
    }

    // Read messages from a specific offset
    public void readFromOffset(int partition, long offset) {
        try (KafkaConsumer<String, String> consumer = createKafkaConsumer()) {
            TopicPartition topicPartition = new TopicPartition(TOPIC, partition);
            consumer.assign(Collections.singletonList(topicPartition));
            consumer.seek(topicPartition, offset); // Start reading from a specific offset

            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
            records.forEach(record -> 
                System.out.println("Offset: " + record.offset() + ", Message: " + record.value()));

            // Manually commit offset
            consumer.commitSync();
        }
    }

    // Read messages from a specific timestamp
    public void readFromTimestamp(int partition, long timestamp) {
        try (KafkaConsumer<String, String> consumer = createKafkaConsumer()) {
            TopicPartition topicPartition = new TopicPartition(TOPIC, partition);
            consumer.assign(Collections.singletonList(topicPartition));

            Map<TopicPartition, Long> timestampToSearch = new HashMap<>();
            timestampToSearch.put(topicPartition, timestamp);
            var offsetsForTimes = consumer.offsetsForTimes(timestampToSearch);

            if (offsetsForTimes.get(topicPartition) != null) {
                long offset = offsetsForTimes.get(topicPartition).offset();
                consumer.seek(topicPartition, offset); // Seek to timestamp offset

                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
                records.forEach(record -> 
                    System.out.println("Timestamp: " + record.timestamp() + ", Message: " + record.value()));

                consumer.commitSync();
            } else {
                System.out.println("No messages found for the given timestamp.");
            }
        }
    }
}
